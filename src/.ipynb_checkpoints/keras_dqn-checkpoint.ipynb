{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCERNS\n",
    "1. list of directions/positions are never emptied during an episode, which makes the simulation slower and slower after each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAS\n",
    "other possible features:\n",
    "1. robot is fallen down or not\n",
    "2. distance to border (& which border?)\n",
    "\n",
    "time optimalization:\n",
    "1. clear last item from history every 4 timesteps (we only use the current and previous state and the one before that)\n",
    "2. interval of states to be interpreted: skip N frames before evaluation next state\n",
    "3. Since the rewards are so sparse, maybe use Imitation learning instead of DQN --> we are \"experts\" since we know the tactic of the blue bot. we can use this to teach our bot how to defeat the other agent.\n",
    "4. rewrite the random action function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from stopwatch import Stopwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "max_distance = 600\n",
    "show_images = False\n",
    "skip_frames = 15\n",
    "state_size =  11 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 10           # Please check the Updates section above for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) # action size: move in x or y direction, or do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(standard_reward, distance):\n",
    "    distance_reward = (max_distance-distance)/max_distance\n",
    "    total_reward = (distance_reward + standard_reward)/ 2\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (agent_path[-1] - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(agent_path[-1] - np.array(agent_path[-2]))[1]\n",
    "    E_X = (enemy_path[-1] - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(enemy_path[-1] - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps, eval_pic, environment):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    info, reward, agent_coord, enemy_coord, following_state = environment.actionLoop(action, 0, eval_pic)\n",
    "    stopwatch.stop()\n",
    "    #print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "    \n",
    "    complete_reward = compute_reward(reward, distance)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    return info, complete_reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy, following_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = AgentEnvironment(size=size, timescale=timescale)\n",
    "def init_environment(agent_here):\n",
    "    #env = AgentEnvironment(size=size, timescale=timescale)\n",
    "    info, reward, state = env.reset() \n",
    "  \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    small_init_state = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "    \n",
    "    #for i in range(3):\n",
    "    action = agent_here.act(small_init_state) # get next action\n",
    "    # action = 3 (if above does not work)\n",
    "    \n",
    "    #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "    #step_number_now = i+1\n",
    "    info, complete_reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, 1, True, env)  \n",
    "    \n",
    "    #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "    #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "    agent_trajectories.append(list(agent_pos))\n",
    "    enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return info, complete_reward, next_state, agent_trajectories, enemy_trajectories, agent_direction, relative_pos_enemy, enemy_direction, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game is finished, \n",
      " your final reward is: 118.42416666666668, duration was 256 timesteps\n",
      "Game is finished, \n",
      " your final reward is: 110.0925, duration was 242 timesteps\n",
      "[[342 158   0   0  46  53  87  13 -23  49   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[0 0 0 0 3 0 0 0 8 0 0]]\n",
      "Game is finished, \n",
      " your final reward is: 89.70416666666665, duration was 198 timesteps\n",
      "[[295 212   0   0  -2 -19 157  15   3  -5   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[  0   0   0   0   3  -1   0   0 -17  -1   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   0   1   0   0 -11  10   0]]\n",
      "Game is finished, \n",
      " your final reward is: 98.38833333333334, duration was 215 timesteps\n",
      "[[298 191   0   0 -23  -5 153  33   9 -28   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[  0   0   0   0   5  -1   0   0 -17  -1   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  0  1  0  0 -7 -5  0]]\n",
      "state that the model will use to predict action: [[450 192   0   0   2  -1  24  76 -25  -4   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2  -1   0   0 -25  -4   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0  2 -6  0]]\n",
      "Game is finished, \n",
      " your final reward is: 97.92999999999999, duration was 213 timesteps\n",
      "[[277 187   0   0  52  -6 243   7 -36   2   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0  2 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -2  0  0 -2 -1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -4 -6  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -4 -6  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5 -1  0  0  1  7  0]]\n",
      "Game is finished, \n",
      " your final reward is: 183.6975, duration was 389 timesteps\n",
      "[[449 220   0   0 -15 -38  36 -48   3   7   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 -8  4  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  6  1  0  0 -2  5  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -3  0  0 11 -5  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  0 -1  0  0 -2  0  0]]\n",
      "Game is finished, \n",
      " your final reward is: 104.45083333333335, duration was 227 timesteps\n",
      "[[162 184   0   0  25   3 450   0 -39  11   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  1  0  0 -3  5  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  6  0  0 -1  4  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  6  0  0 -1  4  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  6  0  0 -1  4  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -2  2  0  0 -5  9  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -2  2  0  0 -5  9  0]]\n",
      "state that the model will use to predict action: [[422 227   0   0  41 -12 -24 -14  51   7   0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 3 2 0 0 7 0 0]]\n",
      "Game is finished, \n",
      " your final reward is: 152.825, duration was 325 timesteps\n",
      "[[298 236   0   0 -36 -30  53   2 -38 -29   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5  0  0  0 -3  0  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0  -1   0   0   0  -9 -32   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  0 -1  0  0 -4  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5 -1  0  0 -3 -1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -6  4  0]]\n",
      "Game is finished, \n",
      " your final reward is: 95.01916666666666, duration was 207 timesteps\n",
      "[[ 585  203    0    0  137  -45 -172  -49  -46  -10    0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  6 -1  0  0 -4 -4  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 -8 -4  0]]\n",
      "state that the model will use to predict action: [[308 174   0   0  28   6 171  16 -33   7   0]]\n",
      "state that the model will use to predict action: [[407 193   0   0  35 -21  55   9  24   4   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2  0  0  0 -6  7  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1  8  0  0  3  4  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1  8  0  0  3  4  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0  -1 -36   0   0   4 -15   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -4  2  0  0  0  3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -4  2  0  0  0  3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -4  2  0  0  0  3  0]]\n",
      "state that the model will use to predict action: [[ 535  256    0    0   36  -14  -85 -107  -48  -17    0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -1 -6  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -1  0  0  3 -1  0]]\n",
      "Game is finished, \n",
      " your final reward is: 125.02166666666668, duration was 269 timesteps\n",
      "[[ 466  206    0    0 -104   68  -18  -18   31   -8    0]]\n",
      "train\n",
      "state that the model will use to predict action: [137, 187, 653, 195, 1, 0, 516, 8, -1, 0, False]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-198aa828f5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mevaluate_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#step(info, reward, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(f\"agent chooses action: {action}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstopwatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studie/NIPS/NIPS-project/src/DQN_Agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"state that the model will use to predict action: {state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "complete_rewards = []\n",
    "for e in range(episode_count):\n",
    "    status, complete_reward, next_state, agent_trajectories, enemy_trajectories, agent_dir, relative_pos_enemy, enemy_dir, environment = init_environment(agent)\n",
    "    small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[-1][0], enemy_trajectories[-1][1], agent_dir[0], agent_dir[1], relative_pos_enemy[0], relative_pos_enemy[1], enemy_dir[0], enemy_dir[1], 0]#\"agent direction\", \"relative position enemy\", \"enemy direction\" ]\n",
    "    small_state = np.reshape(next_small_state, [1, state_size])\n",
    "    \n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    total_timesteps = 1\n",
    "    distances = []\n",
    "    evaluate_frame = False\n",
    "\n",
    "    while done == False:\n",
    "        if (total_timesteps % skip_frames == 0) or (total_timesteps % skip_frames == skip_frames-1):\n",
    "            evaluate_frame = True\n",
    "        else:\n",
    "            evaluate_frame = False\n",
    "            \n",
    "        action = agent.act(small_state) #step(info, reward, state)\n",
    "        #print(f\"agent chooses action: {action}\")\n",
    "        stopwatch = Stopwatch() \n",
    "        stopwatch.start()\n",
    "        status, complete_reward, agent_pos, enemy_pos, agent_dir, enemy_dir, distance, enemy_pos_rel, next_state = do_action(action, total_timesteps, evaluate_frame, environment)   \n",
    "        stopwatch.stop()\n",
    "        #print(f\"Total time for one step: {stopwatch.duration}\")\n",
    "        \n",
    "        total_reward += complete_reward\n",
    "\n",
    "        if status == 1:\n",
    "            print(f\"Game is finished, \\n your final reward is: {total_reward}, duration was {total_timesteps} timesteps\")\n",
    "            done = 1\n",
    "        \n",
    "        agent_trajectories.append(list(agent_pos))\n",
    "        enemy_trajectories.append(list(enemy_pos))\n",
    "        distances.append(distance)\n",
    "        \n",
    "        done_list = [done]\n",
    "        next_small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[1][0], enemy_trajectories[1][1], agent_dir[0], agent_dir[1], enemy_pos_rel[0], enemy_pos_rel[1], enemy_dir[0], enemy_dir[1], done]  \n",
    "    \n",
    "        next_small_state = np.reshape(next_small_state, [1, state_size]) # why?\n",
    "        small_state = np.reshape(small_state, [1, state_size])\n",
    "        \n",
    "        if (total_timesteps % skip_frames == 0):\n",
    "            agent.remember(small_state, action, complete_reward, next_small_state, list(done_list))\n",
    "        \n",
    "        small_state = next_small_state # new small state\n",
    "        total_timesteps += 1\n",
    "        \n",
    "    complete_rewards.append(total_reward)\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size)\n",
    "        print(\"train\")\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        agent.save(model_output_dir + \"weights_\"+ '{:04d}'.format(e) + \".hdf5\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

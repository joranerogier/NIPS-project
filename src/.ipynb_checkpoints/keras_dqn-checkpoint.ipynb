{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCERNS\n",
    "1. list of directions/positions are never emptied during an episode, which makes the simulation slower and slower after each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "    \n",
    "state_size =  11 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 1           # Please check the Updates section above for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "environment = AgentEnvironment()\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) # action size: move in x or y direction, or do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(Agent_path,Enemy_path):\n",
    "    A_X = (Agent_path[-1] - np.array(Agent_path[-2]))[0]\n",
    "    A_Y = -(Agent_path[-1] - np.array(Agent_path[-2]))[1]\n",
    "    E_X = (Enemy_path[-1] - np.array(Enemy_path[-2]))[0]\n",
    "    E_Y = -(Enemy_path[-1] - np.array(Enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action):\n",
    "    info, reward, Agent_coord, Enemy_coord, following_state = environment.simpleCoord(action,0)\n",
    "    if len(environment.Agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        Agent_direction = [1,0] # By definition of facing each other\n",
    "        Enemy_direction = [-1,0]\n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(Agent_coord)- np.array(Enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        Agent_direction, Enemy_direction  = direction(environment.Agent_path, environment.Enemy_path)\n",
    "\n",
    "    rel_pos_enemy = np.array(Enemy_coord) - np.array(Agent_coord)\n",
    "    return info, reward, np.array(Agent_coord), np.array(Enemy_coord), Agent_direction, Enemy_direction, distance, rel_pos_enemy, following_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_environment(env, agent_here):\n",
    "    info, reward, state = env.reset() \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        action = agent_here.act(3) # get next action\n",
    "        #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "        info, reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action)  \n",
    "\n",
    "        #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "        #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "        agent_trajectories.append(list(agent_pos))\n",
    "        enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return info, reward, next_state, agent_trajectories, enemy_trajectories, agent_direction, relative_pos_enemy, enemy_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game is finished, \n",
      " your final reward is: 0\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "episode_count = 500\n",
    "\n",
    "for e in range(episode_count):\n",
    "    status, reward, next_state, agent_trajectories, enemy_trajectories, agent_dir, relative_pos_enemy, enemy_dir = init_environment(environment, agent)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_timesteps = 0\n",
    "    small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[-1][0], enemy_trajectories[-1][1], [agent_dir[0]], [agent_dir[1]], [relative_pos_enemy[0]], [relative_pos_enemy[1]], [enemy_dir[0]], [enemy_dir[1]], done]#\"agent direction\", \"relative position enemy\", \"enemy direction\" ] # optimize init in future !\n",
    "    distances = []\n",
    "\n",
    "    while done == False:    \n",
    "        action = agent.act(small_state) #step(info, reward, state)\n",
    "        #print(f\"agent chooses action: {action}\")\n",
    "        \n",
    "        status, reward, agent_pos, enemy_pos, agent_dir, enemy_dir, distance, enemy_pos_rel, next_state = do_action(action)   \n",
    "\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        if status == 1:\n",
    "            print(f\"Game is finished, \\n your final reward is: {total_reward}, achieved in {total_timesteps} timesteps\")\n",
    "            done = True\n",
    "        \n",
    "        agent_trajectories.append(list(agent_pos))\n",
    "        enemy_trajectories.append(list(enemy_pos))\n",
    "        distances.append(distance)\n",
    "        \n",
    "        done_list = [done]\n",
    "        next_small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[1][0], enemy_trajectories[1][1], agent_dir[0], agent_dir[1], enemy_pos_rel[0], enemy_pos_rel[1], enemy_dir[0], enemy_dir[1], done]  \n",
    "    \n",
    "        next_small_state = np.reshape(next_small_state, [1, state_size]) # why?\n",
    "\n",
    "        agent.remember(small_state, action, reward, next_small_state, list(done_list))\n",
    "        small_state = next_small_state\n",
    "        total_timesteps += 1\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size)\n",
    "        print(\"train\")\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        agent.save(model_output_dir + \"weights_\"+ '{:04d}'.format(e) + \".hdf5\")\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

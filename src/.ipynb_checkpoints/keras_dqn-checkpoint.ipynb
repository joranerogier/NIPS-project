{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCERNS\n",
    "1. list of directions/positions are never emptied during an episode, which makes the simulation slower and slower after each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAS\n",
    "other possible features:\n",
    "1. robot is fallen down or not\n",
    "2. distance to border (& which border?)\n",
    "\n",
    "time optimalization:\n",
    "1. clear last item from history every 4 timesteps (we only use the current and previous state and the one before that)\n",
    "2. interval of states to be interpreted: skip N frames before evaluation next state\n",
    "3. Since the rewards are so sparse, maybe use Imitation learning instead of DQN --> we are \"experts\" since we know the tactic of the blue bot. we can use this to teach our bot how to defeat the other agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from stopwatch import Stopwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "show_images = False\n",
    "skip_frames = 10\n",
    "state_size =  11 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 5           # Please check the Updates section above for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "environment = AgentEnvironment(size=size, timescale=timescale)\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) # action size: move in x or y direction, or do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (agent_path[-1] - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(agent_path[-1] - np.array(agent_path[-2]))[1]\n",
    "    E_X = (enemy_path[-1] - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(enemy_path[-1] - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    info, reward, agent_coord, enemy_coord, following_state = environment.simpleCoord(action, 0, skip_frames, total_steps)\n",
    "    stopwatch.stop()\n",
    "    print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    return info, reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy, following_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_environment(env, agent_here):\n",
    "    info, reward, state = env.reset() \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        action = agent_here.act(3) # get next action\n",
    "        #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "        step_number_now = i+1\n",
    "        info, reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, step_number_now)  \n",
    "\n",
    "        #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "        #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "        agent_trajectories.append(list(agent_pos))\n",
    "        enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return info, reward, next_state, agent_trajectories, enemy_trajectories, agent_direction, relative_pos_enemy, enemy_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'stopwatch' has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d9d4dd4c9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_trajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_trajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos_enemy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-97347ae4e1af>\u001b[0m in \u001b[0;36minit_environment\u001b[0;34m(env, agent_here)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstep_number_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_direction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_direction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos_enemy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_number_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-26a50dbcb774>\u001b[0m in \u001b[0;36mdo_action\u001b[0;34m(action, total_steps)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollowing_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimpleCoord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'stopwatch' has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "for e in range(episode_count):\n",
    "    status, reward, next_state, agent_trajectories, enemy_trajectories, agent_dir, relative_pos_enemy, enemy_dir = init_environment(environment, agent)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_timesteps = 4\n",
    "    small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[-1][0], enemy_trajectories[-1][1], [agent_dir[0]], [agent_dir[1]], [relative_pos_enemy[0]], [relative_pos_enemy[1]], [enemy_dir[0]], [enemy_dir[1]], done]#\"agent direction\", \"relative position enemy\", \"enemy direction\" ]\n",
    "    distances = []\n",
    "\n",
    "    while done == False:    \n",
    "        action = agent.act(small_state) #step(info, reward, state)\n",
    "        #print(f\"agent chooses action: {action}\")\n",
    "        stopwatch = Stopwatch() \n",
    "        stopwatch.start()\n",
    "        status, reward, agent_pos, enemy_pos, agent_dir, enemy_dir, distance, enemy_pos_rel, next_state = do_action(action, total_timesteps)   \n",
    "        stopwatch.stop()\n",
    "        print(f\"Total time for one step: {stopwatch.duration}\")\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        if status == 1:\n",
    "            print(f\"Game is finished, \\n your final reward is: {total_reward}, duration was {total_timesteps} timesteps\")\n",
    "            done = True\n",
    "        \n",
    "        agent_trajectories.append(list(agent_pos))\n",
    "        enemy_trajectories.append(list(enemy_pos))\n",
    "        distances.append(distance)\n",
    "        \n",
    "        done_list = [done]\n",
    "        next_small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[1][0], enemy_trajectories[1][1], agent_dir[0], agent_dir[1], enemy_pos_rel[0], enemy_pos_rel[1], enemy_dir[0], enemy_dir[1], done]  \n",
    "    \n",
    "        next_small_state = np.reshape(next_small_state, [1, state_size]) # why?\n",
    "\n",
    "        agent.remember(small_state, action, reward, next_small_state, list(done_list))\n",
    "        small_state = next_small_state\n",
    "        total_timesteps += 1\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size)\n",
    "        print(\"train\")\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        agent.save(model_output_dir + \"weights_\"+ '{:04d}'.format(e) + \".hdf5\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

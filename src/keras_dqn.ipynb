{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCERNS\n",
    "1. list of directions/positions are never emptied during an episode, which makes the simulation slower and slower after each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAS\n",
    "other possible features:\n",
    "1. robot is fallen down or not\n",
    "2. distance to border (& which border?)\n",
    "\n",
    "time optimalization:\n",
    "1. clear last item from history every 4 timesteps (we only use the current and previous state and the one before that)\n",
    "2. interval of states to be interpreted: skip N frames before evaluation next state\n",
    "3. Since the rewards are so sparse, maybe use Imitation learning instead of DQN --> we are \"experts\" since we know the tactic of the blue bot. we can use this to teach our bot how to defeat the other agent.\n",
    "4. rewrite the random action function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from stopwatch import Stopwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "max_distance = 600\n",
    "show_images = False\n",
    "skip_frames = 15\n",
    "state_size =  13 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 10           # Please check the Updates section above for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "environment = AgentEnvironment(size=size, timescale=timescale)\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) # action size: move in x or y direction, or do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_reward(standard_reward, distance):\n",
    "    distance_reward = (max_distance-distance)/max_distance\n",
    "    total_reward = (distance_reward + standard_reward)/ 2\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (agent_path[-1] - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(agent_path[-1] - np.array(agent_path[-2]))[1]\n",
    "    E_X = (enemy_path[-1] - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(enemy_path[-1] - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps, eval_pic):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    # Normalize the values \n",
    "    info, reward, agent_coord, enemy_coord, _ = environment.actionLoop(action, 0, 1)\n",
    "    # Replace folowing_state with the representation\n",
    "\n",
    "    \n",
    "    stopwatch.stop()\n",
    "    #print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "        \n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "    \n",
    "    complete_reward = compute_reward(reward, distance)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    # Return a value in range 0,1 for following_state\n",
    "    # Nstatus = d[0]\n",
    "    # Ncomplete_reward = d[1]\n",
    "    # Nagent_posx = d[2][0]/700\n",
    "    # Nagent_posy = d[2][1]/700\n",
    "    # Nenemy_posx = d[3][0]/700\n",
    "    # Nenemy_posy = d[3][1]/700\n",
    "    # Nagent_dirx = d[4][0]/60+0.5\n",
    "    # Nagent_diry = d[4][1]/60+0.5\n",
    "    # Nenemy_dirx = d[5][0]/60+0.5\n",
    "    # Nenemy_diry = d[5][1]/60+0.5\n",
    "    # Nenemy_pos_relx = d[7][0]/1400+0.5\n",
    "    # Nenemy_pos_rely = d[7][1]/1400+0.5\n",
    "    # Ndistance = d[6]/700\n",
    "   \n",
    "    d=info, complete_reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy\n",
    "    following_state = d[0],d[1],d[2][0]/700,d[2][1]/700,d[3][0]/700,d[3][1]/700,\\\n",
    "     d[4][0]/60+0.5,d[4][1]/60+0.5,d[5][0]/60+0.5,d[5][1]/60+0.5,\\\n",
    "     d[7][0]/1400+0.5,d[7][1]/1400+0.5,d[6]/700\n",
    "    \n",
    "    return list(following_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_environment(env, agent_here):\n",
    "    info, reward, state = env.reset() \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    small_init_state = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], False]\n",
    "    \n",
    "    #for i in range(3):\n",
    "    action = agent_here.act(small_init_state) # get next action\n",
    "    # action = 3 (if above does not work)\n",
    "    \n",
    "    #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "    #step_number_now = i+1\n",
    "    info, complete_reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, 1, True)  \n",
    "    \n",
    "    #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "    #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "    agent_trajectories.append(list(agent_pos))\n",
    "    enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "complete_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(episode_count,environment, agent):\n",
    "    for e in range(episode_count):\n",
    "        status, complete_reward, next_state, agent_trajectories, enemy_trajectories, agent_dir, relative_pos_enemy, enemy_dir = init_environment(environment, agent)\n",
    "        small_state = [agent_trajectories[-1][0],\n",
    "                       agent_trajectories[-1][1],\n",
    "                       enemy_trajectories[-1][0],\n",
    "                       enemy_trajectories[-1][1], \n",
    "                       [agent_dir[0]], \n",
    "                       [agent_dir[1]], \n",
    "                       [relative_pos_enemy[0]],\n",
    "                       [relative_pos_enemy[1]],\n",
    "                       [enemy_dir[0]], [enemy_dir[1]], False]#\"agent direction\", \"relative position enemy\", \"enemy direction\" ]\n",
    "    \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_timesteps = 1\n",
    "        distances = []\n",
    "        evaluate_frame = False\n",
    "    \n",
    "        while done == False:\n",
    "            if (total_timesteps % skip_frames == 0) or (total_timesteps % skip_frames == skip_frames-1):\n",
    "                evaluate_frame = True\n",
    "            else:\n",
    "                evaluate_frame = False\n",
    "                \n",
    "            action = agent.act(small_state) #step(info, reward, state)\n",
    "            #print(f\"agent chooses action: {action}\")\n",
    "            stopwatch = Stopwatch() \n",
    "            stopwatch.start()\n",
    "            next_state = do_action(action,total_timesteps,evaluate_frame)   \n",
    "            \n",
    "            status=next_state[0]\n",
    "            complete_reward=[1]\n",
    "            \n",
    "            stopwatch.stop()\n",
    "            #print(f\"Total time for one step: {stopwatch.duration}\")\n",
    "            \n",
    "            total_reward += complete_reward\n",
    "    \n",
    "            if status == 1:\n",
    "                print(f\"Game is finished, \\n your final reward is: {total_reward}, duration was {total_timesteps} timesteps\")\n",
    "                done = True\n",
    "            \n",
    "            agent_trajectories.append(list(agent_pos))\n",
    "            enemy_trajectories.append(list(enemy_pos))\n",
    "            distances.append(distance)\n",
    "            \n",
    "            done_list = [done]\n",
    "            next_small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[1][0], enemy_trajectories[1][1], agent_dir[0], agent_dir[1], enemy_pos_rel[0], enemy_pos_rel[1], enemy_dir[0], enemy_dir[1], done]  \n",
    "        \n",
    "            next_small_state = np.reshape(next_small_state, [1, state_size]) # why?\n",
    "    \n",
    "            if (total_timesteps % skip_frames == 0):\n",
    "                agent.remember(small_state, action, complete_reward, next_small_state, list(done_list))\n",
    "            small_state = next_small_state\n",
    "            total_timesteps += 1\n",
    "            \n",
    "        complete_rewards.append(total_reward)\n",
    "    \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.train(batch_size)\n",
    "            print(\"train\")\n",
    "    \n",
    "        if e % 1 == 0:\n",
    "            #agent.save(model_output_dir + \"Weights_\"+ '{:04d}'.format(e) + \".hdf5\")\n",
    "            agent.save( \"Weights.hdf5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import pickle\n",
    "# load the model from disk\n",
    "#nn = pickle.load(open(\"model.sav\", 'rb'))\n",
    "nn = MLPRegressor(\n",
    "     hidden_layer_sizes=(100,100,100,100),  activation='relu', solver='adam', alpha=0.001, batch_size='auto',\n",
    "     learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "     random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "     early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Train the model with \n",
    "NumberOfGames=10\n",
    "epsilon = 1 # zero means model, 1 random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "u=0\n",
    "for i in range(NumberOfGames):\n",
    "    environment.reset() \n",
    "    F_next_state=[]\n",
    "    F_reward=[]\n",
    "    u+=1\n",
    "    j=1\n",
    "    status=0\n",
    "    \n",
    "    rand=True\n",
    "    while status==0 or j<10:  \n",
    "        if np.random.rand() <= epsilon:\n",
    "            rand=True\n",
    "        else:\n",
    "            rand=False\n",
    "            \n",
    "        \n",
    "        if rand:\n",
    "            action=random.randrange(3)\n",
    "\n",
    "\n",
    "\n",
    "        next_state = do_action(action,1,True)\n",
    "        # action=NewAgent.act(1)\n",
    "\n",
    "        status=next_state[0] # Normalized status \n",
    "        reward=next_state[1] # reward\n",
    "        next_state = next_state[2:] # State variables\n",
    "        next_state.append(action/2)\n",
    "\n",
    "        F_next_state.append(next_state)\n",
    "        F_reward.append(reward)\n",
    "        exp_value=[]\n",
    "        if rand:\n",
    "                action=random.randrange(3)\n",
    "        else:\n",
    "            for k in range(3):\n",
    "                ex_reward=F_next_state[-1][:-1]\n",
    "                ex_reward.append(k/2)\n",
    "                ex_reward\n",
    "                prediction=nn.predict(np.array(ex_reward).reshape(1, -1))\n",
    "                exp_value.append(prediction)\n",
    "            action = np.argmax(exp_value)\n",
    "        \n",
    "        j=10\n",
    "        \n",
    "    x = F_next_state\n",
    "    y = F_reward\n",
    "    n = nn.fit(x, y) # Train after each game\n",
    "    filename = 'model.sav'\n",
    "    pickle.dump(nn, open(filename, 'wb'))\n",
    "    print(\"Game number and reward\",u,[\"loose\",\"win\",\"win\",\"win\",\"win\",\"win\"][int(y[-1])])\n",
    "    plt.plot(nn.predict(F_next_state)-F_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'win'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 41
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VPython",
   "language": "python",
   "name": "vpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
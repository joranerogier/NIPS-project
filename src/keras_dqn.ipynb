{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCERNS\n",
    "1. list of directions/positions are never emptied during an episode, which makes the simulation slower and slower after each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAS\n",
    "other possible features:\n",
    "1. robot is fallen down or not\n",
    "2. distance to border (& which border?)\n",
    "\n",
    "time optimalization:\n",
    "1. clear last item from history every 4 timesteps (we only use the current and previous state and the one before that)\n",
    "2. interval of states to be interpreted: skip N frames before evaluation next state\n",
    "3. Since the rewards are so sparse, maybe use Imitation learning instead of DQN --> we are \"experts\" since we know the tactic of the blue bot. we can use this to teach our bot how to defeat the other agent.\n",
    "4. rewrite the random action function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from stopwatch import Stopwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "max_distance = 600\n",
    "show_images = False\n",
    "skip_frames = 15\n",
    "state_size =  13 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 10           # Please check the Updates section above for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "environment = AgentEnvironment(size=size, timescale=timescale)\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) # action size: move in x or y direction, or do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_reward(standard_reward, distance):\n",
    "    distance_reward = (max_distance-distance)/max_distance\n",
    "    total_reward = (distance_reward + standard_reward)/ 2\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (agent_path[-1] - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(agent_path[-1] - np.array(agent_path[-2]))[1]\n",
    "    E_X = (enemy_path[-1] - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(enemy_path[-1] - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps, eval_pic):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    # Normalize the values \n",
    "    info, reward, agent_coord, enemy_coord, _ = environment.actionLoop(action, 0, 1)\n",
    "    # Replace folowing_state with the representation\n",
    "\n",
    "    \n",
    "    stopwatch.stop()\n",
    "    #print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "        \n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "    \n",
    "    complete_reward = compute_reward(reward, distance)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    # Return a value in range 0,1 for following_state\n",
    "    # Nstatus = d[0]\n",
    "    # Ncomplete_reward = d[1]\n",
    "    # Nagent_posx = d[2][0]/700\n",
    "    # Nagent_posy = d[2][1]/700\n",
    "    # Nenemy_posx = d[3][0]/700\n",
    "    # Nenemy_posy = d[3][1]/700\n",
    "    # Nagent_dirx = d[4][0]/60+0.5\n",
    "    # Nagent_diry = d[4][1]/60+0.5\n",
    "    # Nenemy_dirx = d[5][0]/60+0.5\n",
    "    # Nenemy_diry = d[5][1]/60+0.5\n",
    "    # Nenemy_pos_relx = d[7][0]/1400+0.5\n",
    "    # Nenemy_pos_rely = d[7][1]/1400+0.5\n",
    "    # Ndistance = d[6]/700\n",
    "   \n",
    "    d=info, complete_reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy\n",
    "    following_state = d[0],d[1],d[2][0]/700,d[2][1]/700,d[3][0]/700,d[3][1]/700,\\\n",
    "     d[4][0]/60+0.5,d[4][1]/60+0.5,d[5][0]/60+0.5,d[5][1]/60+0.5,\\\n",
    "     d[7][0]/1400+0.5,d[7][1]/1400+0.5,d[6]/700\n",
    "    \n",
    "    return list(following_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_environment(env, agent_here):\n",
    "    info, reward, state = env.reset() \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    small_init_state = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], False]\n",
    "    \n",
    "    #for i in range(3):\n",
    "    action = agent_here.act(small_init_state) # get next action\n",
    "    # action = 3 (if above does not work)\n",
    "    \n",
    "    #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "    #step_number_now = i+1\n",
    "    info, complete_reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, 1, True)  \n",
    "    \n",
    "    #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "    #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "    agent_trajectories.append(list(agent_pos))\n",
    "    enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "#nn = pickle.load(open(\"model.sav\", 'rb'))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "nn = pickle.load(open(\"modelk.sav\", 'rb'))\n",
    "#nn = Sequential()\n",
    "#nn.add(Dense(132, activation='relu', input_dim=12))\n",
    "#nn.add(Dense(132, activation='relu'))\n",
    "#nn.add(Dense(132, activation='relu'))\n",
    "#nn.add(Dense(132, activation='relu'))\n",
    "#nn.add(Dense(132, activation='relu'))\n",
    "#nn.add(Dense(1, activation='linear'))\n",
    "#nn.compile(loss='mse', optimizer=Adadelta()) # originally Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "code_folding": [],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Train the model with \n",
    "NumberOfGames=2\n",
    "epsilon = 0 # zero means model, 1 random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/1\n",
      "661/661 [==============================] - 1s 2ms/step - loss: 0.0389 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Game number and reward 1 win\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a6ddcd38b0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# action=NewAgent.act(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-b39f830b7de2>\u001b[0m in \u001b[0;36mdo_action\u001b[0;34m(action, total_steps, eval_pic)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Normalize the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Replace folowing_state with the representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-projectt/src/VisualModule.py\u001b[0m in \u001b[0;36mactionLoop\u001b[0;34m(self, ACTION, view_image, analyse_frame)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mprevious_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# take last image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_altered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnxt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m#print(f\"Total time for coord: {stopwatch.duration}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-projectt/src/VisualModule.py\u001b[0m in \u001b[0;36mcoord\u001b[0;34m(self, initial_state, action, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstate_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# do action (new \"timestep\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# only get all extra information every 'skip_frames' times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-projectt/src/VisualModule.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-projectt/src/VisualModule.py\u001b[0m in \u001b[0;36m_receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Kudos to Jan for the socket.MSG_WAITALL fix!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSG_WAITALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range"
     ],
     "ename": "IndexError",
     "evalue": "index out of range",
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGHtJREFUeJzt3Xl0XOd53/HvMwtmMNhBgCS4gCApSiS1mJJQyYo225JixXaUpXZbNyd2jp0y9rHTOMtJrKhN3dP0JG3a2mnjk4RJ7CS24+TYjpfKbrTYcp3a1kJqIymKFEmBC7gAxL7N/vaPuYBAGiRAzvBi5s3vc84czL1zce8zg8Ez7zzve99rzjlERMQfkeUOQEREKkuJXUTEM0rsIiKeUWIXEfGMEruIiGeU2EVEPKPELiLiGSV2ERHPKLGLiHgmthwH7ejocD09PctxaBGRmrVnz55zzrnOxbZblsTe09PD7t27l+PQIiI1y8yOLWU7lWJERDyjxC4i4hkldhERzyixi4h4RoldRMQzSuwiIp5RYhcR8YwSu4hIhRwemOTx/WeWO4zKnKBkZg8CfwhEgT93zv1+Jfa7kH39Y1yzspFkPHq1DiFLVCw6xmZytDXUMZ7O0ZyML3dIIsvqY3/3Avv6x9m6uolVzUlWNydpro9RXxejs7GOt25dybq21FWPo+zEbmZR4NPAA8BJ4Dkz+4Zz7pVy932h7x0a5H2feZb2hjrWtdXzL3rXs6Y1yVuvW4mZVeQYf/H/XueVU+P89ju2sqIxUZF91rr9p8YYn8nz1MEB7r22k56OBr60+wRfef4kJ4ZnqItGyBaK/Nv7trD35Cg3rG3hLdet5OCZCda11XPj2hZiUSMejegD2VPpXIHR6RyrW5LLHUrFDEykaUvVkckXeeH4CO0NdZwYnubMWJq9/eMcH57i39y9iUQ8yuBEhlOjM+zrHwdgdUuS4aksB06PMziZwbnSPnc838/XPnLnVY/d3OwRr3QHZncAn3DOvT1YfhjAOfd7F/ud3t5edyVTCvy7r+3l808f/5H191zbSSZXoCkZ5xfv3simjgYS8Sgt9aUW5EQ6B0DTElqUPR//JgARg6O/904Annp1gFt72mhOxsnmi7x0cpQN7SnaG+qIRSPkC0UcEI+WKlvZfJG6WOl+segwo2IfPOUoFh1nJ9IkY1HisQh956aIRY3GRIz2hjrOTWTJFooMTmQYmsowPJXl6aNDfGtv5b5armpOsLmzkdXNSVpScTZ2NLCtq5nrVjct2OLP5ouMzmRJxKKcm8xQH49iBiubkkQji7+mhaKjf2SGM+NpprJ5Mrkik5k8T706wLnJDLdvWkEmX5j7m8UjEdK5Av2jM7xt60rq66Js7GggFomwYUXqvA+mbL5ILGJEFoijWHT0j85wdjw9996YzuaZyRbY1NlIrlDk9FiawYkM2XyRrV2l59/VkqQhUWpvOecoOig6x+GBSTZ2NLD/1Djr2+s5dGaS8XSOnhUNbOtqYipb4PjQNMeHp7l1Qxv9ozNs7Gjgh0eGeGD7qvNeq889fYyx6Szv6V1PS32cwYkM+/rHSCVijE5nKRQdfeemGJzMkCuU8kM8GiERi/DUwQFaU3V0NSdZ3ZLk6aNDvHpmYm7fv/vTN7C2tZ7vvDrAL9zZQ3MyTmdT4rzXZaHXayH//mv7aE3FSecKbOtqZmNHAzeubWE8nSdqxj8eHmQ6W6A5GcPMODE8Tf/oDM7B5s4GTo7MMJMrcOuGNhKxKNGIkYxH2N7VTGuqjmjEcM5xbjLL/lNjfOfVAZ4+OkQ6V+T48PSSYlzIJ35yO79w58a55ULRMZnO88G/eo7JTJ5/+Ng9V7xvM9vjnOtddLsKJPZ3Aw86534xWP554Hbn3Ecv2G4nsBOgu7v71mPHljTlwXlm/whNyRhHBic5OjjFkwfO8vTRIc6OZ35k+2tWNvLPetr54rOlD4MP3LmRqUweh+PHNneQyRdY1Zwkmy/yXN8wuYLjL3/QN/f7rak4o9OlD4XNnQ3cdU0Hx4an+e7Bwbltbu5uZV//GAD3bOlkPJ3jub4ROhoTXLuqkR8cGWLLykZu39RO0cHZsTTZQpFC0ZEvOk4OT5NKxLhv20qODEzy5IEBtnc1s6o5wfcPD3HjuhZ6e9pY01JPqi5K/+gMI1NZzoynyeSLtNTHSdVFGZvJUSg6jg5OEYtGmEjnaE3FyeSKvDYwyZvWt3JqdIbBiR99nS7Htq5mhiYzfPwntvLTO9aSLRQZmc6SqotxZizNquYERwYneeX0BCeGp1nfnmJ4Msu+U2OkcwVSdVFODM8wNpNjdDrLVLYwt++NHQ3sWN/K+rZ69vaPMZHOc+D0+HnbzIpHjZb6OgBuXNvMhhUNvHB8hKGpLLf1tJMtFDk2NM1rAxOkc8VFn1dTIkamUCRXKHKpf4mmRIwtqxp5YPtqdn3vCKm6GN3tKaIRI1soMp3NUxeN8Pzx0ct/cQMPXr+aTL7Adw8NnheLGQvGFosY+eLFg3779avoWdFA/+gMTckYX3z2xKIxRAzaGxIkggbKZCbPeDp3ydfmUhKxCJngg3BrVxMRM5qSMaazBQ6fnWTbmmbuuqYD50oNsZlcgS8886ONuEppb6jjhrUtHBmYpH905qLbtabi3L2lk47GOratbmb7mmaakjEKRcfBMxPkio5kLMLOz+0B4NFfvovrVjfNNfLm+6XP7abv3DSP/WptJPb3AG+/ILHf5pz75Yv9zpW22C8mVyjy3OvDXLu6iU89eWiuVX/j2hb2Bkn3aknVRZkOEs+aliSnxtJzj920roWTIzMkYxHS+SIGTGTytNTH6W4v1dmODU0xNJU97x9mfXs9yViU1wYmFzxmMh5hfVuKRDzCsXPTTGTyRCNGoehoS8W5dUMbyXiphWsYu48NY2bce20n27uaSdVFSeeKdK+oByCXdwxOZmhNxWlMxOhsStDRmKA+HuXEyDR3bFrBdLaUlCv5zcM5x5nxNAdOj3Pg9ASPv3KWowOTTGTyAKxoqOPuLR1s7iz1qbTUx5kMHhuYyDAwnsbMeOH4CEfPTc3tNx41VjYl2dTZwHWrmtiyqpEXT4zyxWdPsK6tnj97Xy9NyRinRtNs6myg6Bwrm94oIRw6O8Hek6W+nGjEOHR2gkLRcWYszeOvnGXfqbFFE9w7b+zi5u5WALpa6pnO5ulsKr2mRwanaEzGaKmPs66tnrGZHCNTWYaCr+6f/X4fAJ1NCQYnSn+Xe7Z08o+vDTISNDT+13tvJh6NMDiZYU/fMGfG03S11HP9mmZ+95sHzotlfgMFoCkZ40P3bqa5Ps7/PThIJl/gXTd1sbY1xeqWBGBsWJFaMDkdPDPBhhUpTo3O0NVSTzpXYN+pMf73S6fo7WlnT98Im1c2sLd/nGeODnH3lk5OjkzT0Zjg2b5hbtvYzuuDU8RjETK5wnmt/fkaEzGKzs39b83qbErw4Xs344DW+ji3bGhjJlso/Q2bEzQl4sSiNteIScajxKMR8sUi/SMzjE7nODue5tDAJC+eGGF7VzNrW1MUneOhHWuYSOf5sc0rmEjnaUzEiEdtSe/5R18+xaaORravab7oNh/63B6Onpvk8V+9d9H9XUyYiT20UsyVGJnKYlbqrV7VnOSHR4eIR42JdJ7r17QwPpMjX3SsaU1ybjJLseioi0XIFx2FYpHbN64gGY+SyRcwjJHpLF0tyYv+sZ1zzOQKc1/9lmoinePUaJqVTQnaGkot0SODk7Sn6ojHIkxn8kxk8qxvS82VeeYfL1UXoxh8C5j/OFze19/lls4VyORKZZFELLLkuIcmM3Nfry/m8MAEmzsby/pwKhYdBeeYyuR5bP8Zfusre897vLMpwb/sXc9vvP26Kz7Gc33D/NUP+vjEQ9fTcUE/z9df7KcpGeNtW1dd9PfPjqfJ5ov0DU1x1zUdmJVKDj88MsS2rlKLM7ZA0l4On/3+64zP5PnwWzbPfeNpS9VhQCwa4Uu7T/DlPSd55vVh/voDt3Hbxvaa7af58Of3cGSwdhJ7DDgE3Af0A88B/9o5t/9ivxNmYhcJQzZfJF8skqpblpmwvVYslhovs30PterDn9/D4YFJnvi1q5/Yy36lnHN5M/so8Bil4Y6fuVRSF/FRXSxCnU4LuSoiEav5pA6lPpKwVOTVcs59C/hWJfYlIuKr8uojS6cmhohICIxSX0cYlNhFRMIQYilGiV1EJCQqxYiIeCTMAcdK7CIiYQmpya7ELiISAjNTKUZExCcqxYiIeEjDHUVEPBLmmadK7CIiIVGNXUTEI8bC8+lfDUrsIiIhCPMqakrsIiIhcSEVY5TYRURCoOGOIiIeUo1dRMQnF7kY+dWgxC4iEgILsRijxC4i4hkldhGREJhpSgEREa9oVIyIiIc0pYCIiEc0CZiIiIc03FFExCOGaUoBERGfqBQjIuIhlWJERDyiFruIiIc03FFExCumUoyIiE9qphRjZu8xs/1mVjSz3koFJSLip9oY7rgP+FngexWIRUTEW2HOFRMr55edcwcg3Iu0iojUKtXYRUQ8YhbeqJhFW+xm9iSweoGHHnHOfX2pBzKzncBOgO7u7iUHKCLigzCvoLRoYnfO3V+JAznndgG7AHp7e8P64BIRqRq60IaIiEdqabjjz5jZSeAO4Jtm9lhlwhIR8U/V1NgvxTn3VeCrFYpFRMRbhkbFiIh4Jcxh4UrsIiIhUeepiIhcESV2EZGQaNpeERGPmBFaZldiFxEJQZhnniqxi4iERKUYERGP1MyZpyIisnQa7igi4pEQ+06V2EVEwqBSjIiIhzRXjIiIRzRXjIiIh1xIVXYldhGREGjaXhER36jzVETEPxruKCLiEc0VIyLiI9XYRUT8YaZRMSIiXgmx71SJXUQkLBruKCLiEc0VIyLiIQ13FBHxiGGaj11ExCcqxYiIeEilGBERj2i4o4iIhzTcUUTEJ7VyoQ0z+wMze9XMXjazr5pZa6UCExHxSS2VYp4AbnDO3QQcAh4uPyQREX+FMeSxrMTunHvcOZcPFp8G1pUfkoiIf2p1uOMHgP9Twf2JiHgnjA7U2GIbmNmTwOoFHnrEOff1YJtHgDzwhUvsZyewE6C7u/uKghURqVWzF9oIY2DMoondOXf/pR43s/cD7wLuc5coHjnndgG7AHp7e8Mapy8iUhXCLMUsmtgvxcweBH4LuNc5N12ZkERE/FVq/17dLF9ujf2PgCbgCTN70cz+pAIxiYh4ZzaVV0Up5lKcc9dUKhAREZ/V6qgYERFZRBijYpTYRURCYLUypYCIiFweF0KVXYldRCREKsWIiHhCnaciInLFlNhFREJgIU7cq8QuIhIi1dhFRDwxW2PXqBgREU/U0hWURETkMqgUIyLiCQ13FBHxVBizOyqxi4iEYO4KStV+MWsREVkalWJERDylUoyIiFw2JXYRkRBpuKOIiCfsjVNPrzoldhGREOjMUxERT2muGBERT2i4o4iIp9R5KiLiidkGu8axi4h4wkKsxSixi4iESHPFiIh4Qp2nIiKeUo1dRMQTc52nGhUjIuIJdZ6KiPip6s88NbP/ZGYvm9mLZva4ma2pVGAiIj6ppbli/sA5d5NzbgfwKPA7FYhJRMRf1V5jd86Nz1tsIJwOXxGRmhPirL3Eyt2Bmf1n4H3AGPDWS2y3E9gJ0N3dXe5hRURqioVYjFm0xW5mT5rZvgVuPwXgnHvEObce+ALw0Yvtxzm3yznX65zr7ezsrNwzEBGpIWEMd1y0xe6cu3+J+/ob4JvAfygrIhERD9XMmadmtmXe4kPAq+WFIyLitzCGO5ZbY/99M7sOKALHgA+VH5KIiH/CPPO0rMTunPvnlQpERMRnNVOKERGRy6NJwEREPFFVwx1FRKRydKENERFfzJ55Wu1TCoiIyNLU0iRgIiJSZZTYRURCYLrQhoiIn1RjFxHxxNyZp9V+BSUREVkanXkqIuIplWJERDyhFruIiKc0V4yIiCdm54rRlAIiIp5QKUZExFMqxYiIeEajYkREPKEpBUREvKXOUxERL2jaXhERT6nGLiLiidkSu0bFiIh4QhezFhHxlEoxIiKe0JmnIiKe0oU2REQ8MXcFJZViRET8oFKMiIin1GIXEfFGjQ13NLPfMDNnZh2V2J+IiK9qovPUzNYDDwDHyw9HRMRPc2ee1kgp5pPAbxLOmbIiIjWpZiYBM7OHgH7n3EsVikdERMoUW2wDM3sSWL3AQ48Avw38+FIOZGY7gZ0A3d3dlxGiiEjtC/NCG4smdufc/QutN7MbgY3AS0HA64Dnzew259yZBfazC9gF0Nvbq7KNiPyTFEaNfdHEfjHOub3AytllM+sDep1z5yoQl4iIV+bOPK2FUTEiIrK4MM88veIW+4Wccz2V2peIiK9qZbijiIgsQnPFiIh4SpfGExHxxOyl8VwItRgldhGRMKgUIyLiJ5ViREQ8UTNzxYiIyOXRcEcREU+8MVeMOk9FRLygUoyIiKdUihER8YTOPBUR8ZSGO4qIeOKNM0+v/rGU2EVEQqBSjIiIpzRXjIiIJzTcUUTEU+o8FRHxRdBkV+epiIgnLMRijBK7iEiInOaKERHxg4Y7ioj4SjV2ERE/hDdprxK7iEgoLMRajBK7iEiINNxRRMQT6jwVEfGUhjuKiHhirvNUpRgRET+oFCMi4qmqH+5oZp8ws34zezG4vaNSgYmI+CW8JnusAvv4pHPuv1VgPyIi3tOFNkREPNHeUMc7b+yiozFx1Y9VicT+UTN72cw+Y2ZtFdifiIh3NnY08Omfu4Ub1rZc9WMtmtjN7Ekz27fA7aeAPwY2AzuA08B/v8R+dprZbjPbPTg4WLEnICIi57NK1XvMrAd41Dl3w2Lb9vb2ut27d1fkuCIi/1SY2R7nXO9i25U7KqZr3uLPAPvK2Z+IiJSv3FEx/9XMdlAamtkH/FLZEYmISFnKSuzOuZ+vVCAiIlIZGu4oIuIZJXYREc8osYuIeKZiwx0v66Bmg8CxK/z1DuBcBcMJi+IOX63GrrjDVUtxb3DOdS620bIk9nKY2e6ljOOsNoo7fLUau+IOV63GfSkqxYiIeEaJXUTEM7WY2HctdwBXSHGHr1ZjV9zhqtW4L6rmauwiInJptdhiFxGRS6ipxG5mD5rZQTM7bGYfX+545gvmox8ws33z1rWb2RNm9lrwsy1Yb2b2P4Pn8bKZ3bKMca83s6fM7ICZ7TezX6mF2M0saWbPmtlLQdz/MVi/0cyeCeL+OzOrC9YnguXDweM9yxH3vPijZvaCmT1aK3GbWZ+Z7Q0ug7k7WFfV75MgllYz+7KZvRq8z++ohbjLUTOJ3cyiwKeBnwC2A+81s+3LG9V5/hJ48IJ1Hwe+7ZzbAnw7WIbSc9gS3HZSmtd+ueSBX3fObQPeDHwkeF2rPfYM8Dbn3JsoXQ/gQTN7M/BfKF2ucQswAnww2P6DwIhz7hrgk8F2y+lXgAPzlmsl7rc653bMGx5Y7e8TgD8E/sE5txV4E6XXvRbivnLOuZq4AXcAj81bfhh4eLnjuiDGHmDfvOWDQFdwvws4GNz/U+C9C2233Dfg68ADtRQ7kAKeB26ndKJJ7ML3DPAYcEdwPxZsZ8sU7zpKyeRtwKOUrnJcC3H3AR0XrKvq9wnQDLx+4WtW7XGXe6uZFjuwFjgxb/lksK6arXLOnQYIfq4M1lflcwm+5t8MPEMNxB6UM14EBoAngCPAqHMuv0Bsc3EHj48BK8KNeM6ngN8EisHyCmojbgc8bmZ7zGxnsK7a3yebgEHgs0Hp68/NrIHqj7sstZTYbYF1tTqkp+qei5k1Al8BPuacG7/UpgusW5bYnXMF59wOSi3g24BtC20W/KyKuM3sXcCAc27P/NULbFpVcQfudM7dQqlc8REzu+cS21ZL3DHgFuCPnXM3A1O8UXZZSLXEXZZaSuwngfXzltcBp5YplqU6O3uVqeDnQLC+qp6LmcUpJfUvOOf+PlhdE7EDOOdGge9S6iNoNbPZ6wzMj20u7uDxFmA43EgBuBN4yMz6gL+lVI75FNUfN865U8HPAeCrlD5Mq/19chI46Zx7Jlj+MqVEX+1xl6WWEvtzwJZg9EAd8K+AbyxzTIv5BvD+4P77KdWvZ9e/L+iBfzMwNvu1MGxmZsBfAAecc/9j3kNVHbuZdZpZa3C/HrifUqfYU8C7g80ujHv2+bwb+I4Liqhhcs497Jxb55zrofQe/o5z7ueo8rjNrMHMmmbvAz9O6VKYVf0+cc6dAU6Y2XXBqvuAV6jyuMu23EX+y7kB7wAOUaqlPrLc8VwQ2xeB00CO0qf+BynVQr8NvBb8bA+2NUojfI4Ae4HeZYz7LkpfNV8GXgxu76j22IGbgBeCuPcBvxOs3wQ8CxwGvgQkgvXJYPlw8PimKnjPvIXSBeCrPu4gvpeC2/7Z/79qf58EsewAdgfvla8BbbUQdzk3nXkqIuKZWirFiIjIEiixi4h4RoldRMQzSuwiIp5RYhcR8YwSu4iIZ5TYRUQ8o8QuIuKZ/w/yo27DyDJKxQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u=0\n",
    "for i in range(NumberOfGames):\n",
    "    environment.reset() \n",
    "    F_next_state=[]\n",
    "    F_reward=[]\n",
    "    prd=[]\n",
    "    u+=1\n",
    "    j=1\n",
    "    status=0\n",
    "    \n",
    "    rand=True\n",
    "    while status==0 or j<10:  \n",
    "        if np.random.rand() <= epsilon:\n",
    "            rand=True\n",
    "        else:\n",
    "            rand=False\n",
    "            \n",
    "        \n",
    "        if rand:\n",
    "            action=random.randrange(3)\n",
    "\n",
    "\n",
    "\n",
    "        next_state = do_action(action,1,True)\n",
    "        # action=NewAgent.act(1)\n",
    "\n",
    "        status=next_state[0] # Normalized status \n",
    "        reward=next_state[1] # reward\n",
    "        next_state = next_state[2:] # State variables\n",
    "        next_state.append(action/2)\n",
    "\n",
    "        F_next_state.append(next_state)\n",
    "        F_reward.append(reward)\n",
    "        exp_value=[]\n",
    "        if rand:\n",
    "                action=random.randrange(3)\n",
    "        else:\n",
    "            for k in range(3):\n",
    "                ex_reward=F_next_state[-1][:-1]\n",
    "                ex_reward.append(k/2)\n",
    "                ex_reward\n",
    "                prediction=nn.predict(np.array(ex_reward).reshape(1, -1))\n",
    "                \n",
    "                exp_value.append(prediction)\n",
    "            action = np.argmax(exp_value)\n",
    "        \n",
    "        j=10\n",
    "        \n",
    "    x = F_next_state\n",
    "    y = F_reward\n",
    "    nn.fit(np.array(x), np.array(y)) # Train after each game\n",
    "    filename = 'modelk.sav'\n",
    "    pickle.dump(nn, open(filename, 'wb'))\n",
    "    graph_pred=[nn.predict(np.array(F_next_state[i]).reshape(1, -1))[0][0] for i in range(len(F_next_state))]\n",
    "    plt.plot(np.array(graph_pred)-np.array(y))\n",
    "    print(\"Game number and reward\",u,[\"loose\",\"win\",\"win\",\"win\",\"win\",\"win\"][int(y[-1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VPython",
   "language": "python",
   "name": "vpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
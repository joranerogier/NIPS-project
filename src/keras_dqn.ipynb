{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCERNS\n",
    "1. list of directions/positions are never emptied during an episode, which makes the simulation slower and slower after each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAS\n",
    "other possible features:\n",
    "1. robot is fallen down or not\n",
    "2. distance to border (& which border?)\n",
    "\n",
    "time optimalization:\n",
    "1. clear last item from history every 4 timesteps (we only use the current and previous state and the one before that)\n",
    "2. interval of states to be interpreted: skip N frames before evaluation next state\n",
    "3. Since the rewards are so sparse, maybe use Imitation learning instead of DQN --> we are \"experts\" since we know the tactic of the blue bot. we can use this to teach our bot how to defeat the other agent.\n",
    "4. rewrite the random action function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from stopwatch import Stopwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "max_distance = 600\n",
    "show_images = False\n",
    "skip_frames = 15\n",
    "state_size =  11 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 10           # Please check the Updates section above for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) # action size: move in x or y direction, or do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(standard_reward, distance):\n",
    "    distance_reward = (max_distance-distance)/max_distance\n",
    "    total_reward = (distance_reward + standard_reward)/ 2\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (agent_path[-1] - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(agent_path[-1] - np.array(agent_path[-2]))[1]\n",
    "    E_X = (enemy_path[-1] - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(enemy_path[-1] - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps, eval_pic, environment):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    info, reward, agent_coord, enemy_coord, following_state = environment.actionLoop(action, 0, eval_pic)\n",
    "    stopwatch.stop()\n",
    "    #print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "    \n",
    "    complete_reward = compute_reward(reward, distance)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    return info, complete_reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy, following_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = AgentEnvironment(size=size, timescale=timescale)\n",
    "def init_environment(agent_here):\n",
    "    #env = AgentEnvironment(size=size, timescale=timescale)\n",
    "    info, reward, state = env.reset() \n",
    "  \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    small_init_state = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "    \n",
    "    #for i in range(3):\n",
    "    action = agent_here.act(small_init_state) # get next action\n",
    "    # action = 3 (if above does not work)\n",
    "    \n",
    "    #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "    #step_number_now = i+1\n",
    "    info, complete_reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, 1, True, env)  \n",
    "    \n",
    "    #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "    #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "    agent_trajectories.append(list(agent_pos))\n",
    "    enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return info, complete_reward, next_state, agent_trajectories, enemy_trajectories, agent_direction, relative_pos_enemy, enemy_direction, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game is finished, \n",
      " your final reward is: 99.85583333333335, duration was 217 timesteps\n",
      "Game is finished, \n",
      " your final reward is: 93.53249999999998, duration was 204 timesteps\n",
      "Game is finished, \n",
      " your final reward is: 111.39666666666668, duration was 243 timesteps\n",
      "[[170 176   0   0  33  11 437  12 -46   7   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5  2  0  0 -1 -6  0]]\n",
      "state that the model will use to predict action: [[ 677  128    0    0    1    1 -193   35    8   -7    0]]\n",
      "Game is finished, \n",
      " your final reward is: 97.84083333333336, duration was 214 timesteps\n",
      "[[225 188   0   0  59  -6 341  -5 -39   1   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -1  0  0  8 -2  0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 2 0 0 0 4 0 0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0 -5  6  0]]\n",
      "state that the model will use to predict action: [[354 235   0   0   3  -2 -39  24   4  -2   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5  0  0  0  1 10  0]]\n",
      "Game is finished, \n",
      " your final reward is: 102.54166666666669, duration was 223 timesteps\n",
      "[[573 250   0   0  47  -1 -35  -8  37  -2   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[  0   0   0   0   5   0   0   0 -12   0   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1  0  0  0  9  1  0]]\n",
      "state that the model will use to predict action: [[352 172   0   0   5  -1  82  22  -2  -7   0]]\n",
      "state that the model will use to predict action: [[297 209   0   0  -5  -9   5  -1 -32  -5   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0 11  8  0]]\n",
      "Game is finished, \n",
      " your final reward is: 149.88250000000002, duration was 320 timesteps\n",
      "[[163 181   0   0  26   6 449   8 -39   7   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[0 0 0 0 5 0 0 0 7 0 0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5  1  0  0  0 -1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  2  0  0  5 -1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  2  0  0  5 -1  0]]\n",
      "Game is finished, \n",
      " your final reward is: 85.88833333333334, duration was 189 timesteps\n",
      "[[225 188   0   0  59  -6 341  -5 -39   1   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[  0   0   0   0   6   0   0   0 -13   0   0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 3 0 0 0 1 3 0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1 -1  0  0  5 -5  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -3 -1  0  0  4  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1 -4  0  0 -2  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1  4  0  0  2 -3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2  2  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0  5 -9  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -1  0  0  2  6  0]]\n",
      "Game is finished, \n",
      " your final reward is: 201.49249999999995, duration was 424 timesteps\n",
      "[[474 264   0   0  45  -4 -29  -5  56   2   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 0 1 0 0 0 5 0]]\n",
      "state that the model will use to predict action: [[306 179   0   0  26   1 170  34 -45  -3   0]]\n",
      "state that the model will use to predict action: [[372 178   0   0  63   0  61  42 -26  -8   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0  2 -1  0]]\n",
      "Game is finished, \n",
      " your final reward is: 100.4508333333333, duration was 219 timesteps\n",
      "[[498 154   0   0  42   8 -48  77 -15  15   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   4  -1   0   0 -14  -1   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   3   0   0   0 -15   0   0]]\n",
      "state that the model will use to predict action: [[265 211   0   0  42 -15 249  -5 -41  -8   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0  8 -4  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0  -2   0   0   0 -24   0   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -1  0  0  0  1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -5  0  0 -4 -8  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -2  0  0 12 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2  4  0  0  3 -6  0]]\n",
      "Game is finished, \n",
      " your final reward is: 146.93416666666667, duration was 312 timesteps\n",
      "[[573 250   0   0  47  -1 -35  -8  37  -2   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0  1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0 -3 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0  0 18  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0  5  0  0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 4 0 0 0 4 0 0]]\n",
      "Game is finished, \n",
      " your final reward is: 87.34333333333333, duration was 191 timesteps\n",
      "[[287 220   0   0  21  -7 177 -24 -55   6   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 6 0 0 0 3 2 0]]\n",
      "state that the model will use to predict action: [[224 174   0   0   1   0 326   6  -4  20   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 19 -7  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 19 -7  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -1  0  0 -9 25  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5 -1  0  0 -3 -8  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5 -1  0  0 -3 -8  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0  1 10  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0  1 10  0]]\n",
      "Game is finished, \n",
      " your final reward is: 121.64333333333333, duration was 262 timesteps\n",
      "[[474 264   0   0  45  -4 -29  -5  56   2   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0 -6  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  1  0  0  0 -1  0]]\n",
      "state that the model will use to predict action: [[405 223   0   0   2  -3  63 -15  -1   6   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -3  0  0 -1  6  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   1  -1   0   0 -36 -25   0]]\n",
      "state that the model will use to predict action: [[489 241   0   0   4  -3 -39  -8 -24  17   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   4  -3   0   0 -24  17   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   4  -3   0   0 -24  17   0]]\n",
      "state that the model will use to predict action: [[526 247   0   0  37  -6 -82 -72  -6  58   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -2  0  0 -9  2  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 -96 130   0   0  -4  -6   0]]\n",
      "Game is finished, \n",
      " your final reward is: 94.63750000000003, duration was 207 timesteps\n",
      "[[258 216   0   0  33 -20 267  -6 -42  -9   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2  0  0  0 -2  5  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2  13   0   0 -13   0   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2  13   0   0 -13   0   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2  13   0   0 -13   0   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -3  0  0  9 -4  0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state that the model will use to predict action: [[ 0  0  0  0  0  1  0  0  2 -7  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0  -1  -2   0   0   0 -11   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  1  0  0  8 -2  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0  -8 -12   0   0  -8  -2   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  0  4  0  0 14  8  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  0  4  0  0 14  8  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -3  0  0  0 -2  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0 -3  0  0  0 -2  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -1  0  0  5  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -1  0  0  5  0  0]]\n",
      "Game is finished, \n",
      " your final reward is: 151.41833333333335, duration was 321 timesteps\n",
      "[[ 676  129    0    0  211    5 -200   27   40   20    0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -2  0  0  2  9  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0 -1  1  0]]\n",
      "state that the model will use to predict action: [[403 201   0   0  36   2   0 -27 -33  18   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0  4 -3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -2  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -2  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -1  0  0 -2  0  0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 3 0 0 0 2 0 0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 3 0 0 0 2 0 0]]\n",
      "state that the model will use to predict action: [[567 159   0   0  41 -11 -23 -15  49 -12   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0 -7 -3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0 -7 -3  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 -32  54   0   0   9   0   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 -32  54   0   0   9   0   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   3  -1   0   0 -21  -2   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0 -1  0  0]]\n",
      "Game is finished, \n",
      " your final reward is: 110.41583333333337, duration was 239 timesteps\n",
      "[[222 195   0   0  58  -6 348   6 -29  -5   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0 -3  6  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0 -3  6  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0 -3  6  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1 -2  0  0 -3  6  0]]\n",
      "state that the model will use to predict action: [[288 225   0   0   2  -1 190 -28  17  -8   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5 -1  0  0  1 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  5 -1  0  0  0 -6  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   5   1   0   0 -12   3   0]]\n",
      "state that the model will use to predict action: [[507 265   0   0   2  -3 -24 -17  -5  -2   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -3  0  0 -5 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -2  0  0  1 -3  0]]\n",
      "Game is finished, \n",
      " your final reward is: 88.18333333333332, duration was 193 timesteps\n",
      "[[225 188   0   0  59  -6 341  -5 -39   1   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[  0   0   0   0   6   0   0   0 -15   3   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0 -5  5  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  1  0  0 -5  5  0]]\n",
      "state that the model will use to predict action: [[368 166   0   0  63   0  67  32 -19 -21   0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 2 0 0 0 1 0 0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 2 0 0 0 1 0 0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 2 0 0 0 1 0 0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  0  1  0  0  1 -3  0]]\n",
      "state that the model will use to predict action: [[ 540  159    0    0   32   12 -109   73  -45    9    0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -1  0  0 -8  5  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -1  0  0 -8  5  0]]\n",
      "state that the model will use to predict action: [[   0    0    0    0 -134    0    0    0    2  -10    0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0  2 -1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3 -1  0  0 -1  2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -6  0  0  5  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -6  0  0  5  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  2 -6  0  0  5  1  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4 -1  0  0  3 -1  0]]\n",
      "Game is finished, \n",
      " your final reward is: 109.20333333333333, duration was 236 timesteps\n",
      "[[374 114   0   0 -76   8 -28  53  30  -5   0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -5 -3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -5 -3  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 -6  1  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   4   1   0   0 -21   1   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   4   1   0   0 -21   1   0]]\n",
      "state that the model will use to predict action: [[409 191   0   0   5   2  62   8   3   4   0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 5 2 0 0 3 4 0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2   0   0   0  -4 -10   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 123   0   0   0   3   5   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 123   0   0   0   3   5   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 123   0   0   0   3   5   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0 123   0   0   0   3   5   0]]\n",
      "Game is finished, \n",
      " your final reward is: 93.54749999999999, duration was 204 timesteps\n",
      "[[ 527  122    0    0   22   16 -145   82  -20   15    0]]\n",
      "train\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  1  0  0  0 -1  0  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 -4 -2  0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2   0   0   0  -3 -37   0]]\n",
      "state that the model will use to predict action: [[  0   0   0   0   2   0   0   0  -3 -37   0]]\n",
      "state that the model will use to predict action: [[369 210   0   0   3   0  70   1   0  -2   0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0  0 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0  0 -2  0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  3  0  0  0  0 -2  0]]\n",
      "state that the model will use to predict action: [[ 540  204    0    0   34   10 -111    9  -57   22    0]]\n",
      "state that the model will use to predict action: [[ 0  0  0  0  4  0  0  0 -2 -3  0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 0 2 0 0 1 6 0]]\n",
      "state that the model will use to predict action: [[ 632  185    0    0    0    1 -260   55   -2   -2    0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state that the model will use to predict action: [[ 726  138    0    0   19   21 -357   43   55   12    0]]\n",
      "state that the model will use to predict action: [[0 0 0 0 2 2 0 0 4 3 0]]\n",
      "Game is finished, \n",
      " your final reward is: 105.29916666666666, duration was 232 timesteps\n",
      "[[292 322   0   0 -16 -77 -12 -76 -38 -17   0]]\n",
      "train\n",
      "state that the model will use to predict action: [137, 187, 667, 195, 1, 0, 530, 8, -1, 0, 0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0d59627e6c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mevaluate_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#step(info, reward, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(f\"agent chooses action: {action}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mstopwatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studie/NIPS/NIPS-project/src/DQN_Agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"state that the model will use to predict action: {state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nips/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "complete_rewards = []\n",
    "for e in range(episode_count):\n",
    "    status, complete_reward, next_state, agent_trajectories, enemy_trajectories, agent_dir, relative_pos_enemy, enemy_dir, environment = init_environment(agent)\n",
    "    small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[-1][0], enemy_trajectories[-1][1], agent_dir[0], agent_dir[1], relative_pos_enemy[0], relative_pos_enemy[1], enemy_dir[0], enemy_dir[1], 0]#\"agent direction\", \"relative position enemy\", \"enemy direction\" ]\n",
    "    \n",
    "    done = 0\n",
    "    total_reward = 0\n",
    "    total_timesteps = 1\n",
    "    distances = []\n",
    "    evaluate_frame = False\n",
    "\n",
    "    while done == False:\n",
    "        if (total_timesteps % skip_frames == 0) or (total_timesteps % skip_frames == skip_frames-1):\n",
    "            evaluate_frame = True\n",
    "        else:\n",
    "            evaluate_frame = False\n",
    "            \n",
    "        action = agent.act(small_state) #step(info, reward, state)\n",
    "        #print(f\"agent chooses action: {action}\")\n",
    "        stopwatch = Stopwatch() \n",
    "        stopwatch.start()\n",
    "        status, complete_reward, agent_pos, enemy_pos, agent_dir, enemy_dir, distance, enemy_pos_rel, next_state = do_action(action, total_timesteps, evaluate_frame, environment)   \n",
    "        stopwatch.stop()\n",
    "        #print(f\"Total time for one step: {stopwatch.duration}\")\n",
    "        \n",
    "        total_reward += complete_reward\n",
    "\n",
    "        if status == 1:\n",
    "            print(f\"Game is finished, \\n your final reward is: {total_reward}, duration was {total_timesteps} timesteps\")\n",
    "            done = 1\n",
    "        \n",
    "        agent_trajectories.append(list(agent_pos))\n",
    "        enemy_trajectories.append(list(enemy_pos))\n",
    "        distances.append(distance)\n",
    "        \n",
    "        done_list = [done]\n",
    "        next_small_state = [agent_trajectories[-1][0], agent_trajectories[-1][1], enemy_trajectories[1][0], enemy_trajectories[1][1], agent_dir[0], agent_dir[1], enemy_pos_rel[0], enemy_pos_rel[1], enemy_dir[0], enemy_dir[1], done]  \n",
    "    \n",
    "        next_small_state = np.reshape(next_small_state, [1, state_size]) # why?\n",
    "        small_state = np.reshape(small_state, [1, state_size])\n",
    "        \n",
    "        if (total_timesteps % skip_frames == 0):\n",
    "            agent.remember(small_state, action, complete_reward, next_small_state, list(done_list))\n",
    "        \n",
    "        small_state = next_small_state # new small state\n",
    "        total_timesteps += 1\n",
    "        \n",
    "    complete_rewards.append(total_reward)\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size)\n",
    "        print(\"train\")\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        agent.save(model_output_dir + \"weights_\"+ '{:04d}'.format(e) + \".hdf5\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

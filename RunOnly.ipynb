{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "max_distance = 600\n",
    "show_images = False\n",
    "skip_frames = 15\n",
    "state_size =  13 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 6           # Please check the Updates section above for more details\n",
    "environment = AgentEnvironment(size=size, timescale=timescale)\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) \n",
    "\n",
    "def compute_reward(standard_reward, distance):\n",
    "    distance_reward = (max_distance-distance)/max_distance\n",
    "    total_reward = (distance_reward + standard_reward)/ 2\n",
    "    return total_reward\n",
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (np.array(agent_path[-1]) - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(np.array(agent_path[-1]) - np.array(agent_path[-2]))[1]\n",
    "    E_X = (np.array(enemy_path[-1]) - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(np.array(enemy_path[-1]) - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps, eval_pic):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    # Normalize the values \n",
    "    info, reward, agent_coord, enemy_coord, _ = environment.actionLoop(action, 0, 1)\n",
    "    # Replace folowing_state with the representation\n",
    "\n",
    "    \n",
    "    stopwatch.stop()\n",
    "    #print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "        \n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "    \n",
    "    complete_reward = compute_reward(reward, distance)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    # Return a value in range 0,1 for following_state\n",
    "    # Nstatus = d[0]\n",
    "    # Ncomplete_reward = d[1]\n",
    "    # Nagent_posx = d[2][0]/700\n",
    "    # Nagent_posy = d[2][1]/700\n",
    "    # Nenemy_posx = d[3][0]/700\n",
    "    # Nenemy_posy = d[3][1]/700\n",
    "    # Nagent_dirx = d[4][0]/60+0.5\n",
    "    # Nagent_diry = d[4][1]/60+0.5\n",
    "    # Nenemy_dirx = d[5][0]/60+0.5\n",
    "    # Nenemy_diry = d[5][1]/60+0.5\n",
    "    # Nenemy_pos_relx = d[7][0]/1400+0.5\n",
    "    # Nenemy_pos_rely = d[7][1]/1400+0.5\n",
    "    # Ndistance = d[6]/700\n",
    "   \n",
    "    d=info, complete_reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy\n",
    "    following_state = d[0],d[1],d[2][0]/700,d[2][1]/700,d[3][0]/700,d[3][1]/700,\\\n",
    "     d[4][0]/60+0.5,d[4][1]/60+0.5,d[5][0]/60+0.5,d[5][1]/60+0.5,\\\n",
    "     d[7][0]/1400+0.5,d[7][1]/1400+0.5,d[6]/700\n",
    "    \n",
    "    return list(following_state)\n",
    "def init_environment(env, agent_here):\n",
    "    info, reward, state = env.reset() \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    small_init_state = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], False]\n",
    "    \n",
    "    #for i in range(3):\n",
    "    action = agent_here.act(small_init_state) # get next action\n",
    "    # action = 3 (if above does not work)\n",
    "    \n",
    "    #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "    #step_number_now = i+1\n",
    "    info, complete_reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, 1, True)  \n",
    "    \n",
    "    #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "    #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "    agent_trajectories.append(list(agent_pos))\n",
    "    enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return \n",
    "def view(agent_path,enemy_path):\n",
    "\n",
    "    x_a,y_a,x_e,y_e = xyvalues(agent_path,enemy_path)\n",
    "    A_dir=[]\n",
    "    E_dir=[]\n",
    "    Initial_a_dir = [1,0] # By definition they are facing each other\n",
    "    Initial_e_dir = [-1,0]\n",
    "    Game_1 = pd.DataFrame([x_a,y_a,x_e,y_e]).T\n",
    "    Game = Game_1.rename(columns={0: \"A_x_comp\", 1: \"A_y_comp\",2: \"E_x_comp\", 3: \"E_y_comp\"})\n",
    "    interval=1 # The interval difference to consider a trajectory \n",
    "    for i in range(len(Game)-interval):\n",
    "    \n",
    "        Agent_dir = pd.DataFrame(Game.iloc[i]-Game.iloc[i-interval]).iloc[0:2].values\n",
    "        Enemy_dir = pd.DataFrame(Game.iloc[i]-Game.iloc[i-interval]).iloc[2:4].values\n",
    "        \n",
    "        Agent_dir=(Initial_a_dir[0]+normalize(Agent_dir,axis=0)[0][0])/2,(Initial_a_dir[1]+normalize(Agent_dir,axis=0)[1][0])/2\n",
    "        \n",
    "        Enemy_dir=(Initial_e_dir[0]+normalize(Enemy_dir,axis=0)[0][0])/2,(Initial_e_dir[1]+normalize(Enemy_dir,axis=0)[1][0])/2\n",
    "    \n",
    "        A_dir.append(Agent_dir)\n",
    "        E_dir.append(Enemy_dir) # The direction vector is stored in A_dir, E_dir\n",
    "        \n",
    "    for time in range(len(A_dir)):\n",
    "        V = np.array([E_dir[time]])\n",
    "        origin = Game[[\"E_x_comp\",\"E_y_comp\"]].iloc[time].values[0],Game[[\"E_x_comp\",\"E_y_comp\"]].iloc[time].values[1] # origin point\n",
    "        plt.quiver(*origin, V[:,0], V[:,1], color=['b'], scale=30)\n",
    "    \n",
    "        VA = np.array([ A_dir[time]])\n",
    "        origin = Game[[\"A_x_comp\",\"A_y_comp\"]].iloc[time].values[0],Game[[\"A_x_comp\",\"A_y_comp\"]].iloc[time].values[1] # origin point\n",
    "        plt.quiver(*origin, VA[:,0], VA[:,1], color=['r'], scale=30)\n",
    "    \n",
    "    plt.ylim(300, 768)\n",
    "    plt.xlim(0, 768)\n",
    "    plt.savefig('result_{}.png'.format(int(len(full_games_y))))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = AgentEnvironment(size=size, timescale=timescale)\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncoment the model to you want to use and run :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model with \n",
    "NumberOfGames=50\n",
    "epsilon = 0 # zero means model, 1 random.\n",
    "percentage=[]\n",
    " \n",
    "# To load a decent trained model \n",
    "82.3529411764706 # Model 'nn_82%_50games_8X150n.sav'  loss in training 0.0228, b=100, epoch=2\n",
    "#nn_3 = pickle.load(open('nn_82%_50games_8X150n.sav' , 'rb'))\n",
    "78  #Model 'nn_78%_games_11X150n.sav' loss in training 0.228 b=10000 epoch=1\n",
    "#nn_3 = pickle.load(open('nn_78%_games_11X150n.sav' , 'rb'))\n",
    "72  #Model 'nn_72%_games_11X150n.sav' loss in training 0.0171 b=100, epoch=1\n",
    "#nn_3 = pickle.load(open('nn_72%_games_11X150n.sav' , 'rb'))\n",
    "64  #Model 'nn_60%_50games_8X150n.sav' loss in training 0.0208 b=100, epoch=10\n",
    "#nn_3 = pickle.load(open('nn_60%_50games_8X150n.sav' , 'rb'))\n",
    "66  #Model 'nn_%_games_8X150n.sav' loss in training 0.0192 b=100, epoch=20\n",
    "\n",
    "nn_3 = pickle.load(open('nn_66%_games_8X150n.sav' , 'rb'))\n",
    "\n",
    "model_game=nn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number 1 win\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d7fc597395d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# action=NewAgent.act(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b437dc5a418b>\u001b[0m in \u001b[0;36mdo_action\u001b[0;34m(action, total_steps, eval_pic)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Normalize the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Replace folowing_state with the representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36mactionLoop\u001b[0;34m(self, ACTION, view_image, analyse_frame)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mprevious_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# take last image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_altered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnxt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m#print(f\"Total time for coord: {stopwatch.duration}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36mcoord\u001b[0;34m(self, initial_state, action, image)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstate_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# do action (new \"timestep\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# only get all extra information every 'skip_frames' times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36m_receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "u=0\n",
    "for i in range(NumberOfGames):\n",
    "    environment.reset() \n",
    "    F_next_state=[]\n",
    "    \n",
    "    F_reward=[]\n",
    "    prd=[]\n",
    "    u+=1\n",
    "    j=1\n",
    "    status=0\n",
    "    action=0\n",
    "    rand=True\n",
    "    while status==0 or j<10:  \n",
    "        if np.random.rand() <= epsilon:\n",
    "            rand=True\n",
    "        else:\n",
    "            rand=False\n",
    "            \n",
    "        \n",
    "        if rand:\n",
    "            action=random.randrange(3)\n",
    "\n",
    "\n",
    "\n",
    "        next_state = do_action(action,1,True)\n",
    "        # action=NewAgent.act(1)\n",
    "\n",
    "        status=next_state[0] # Normalized status \n",
    "        reward=next_state[1] # reward\n",
    "        next_state = next_state[2:] # State variables\n",
    "        next_state.append(action/2)\n",
    "\n",
    "        F_next_state.append(next_state)\n",
    "        F_reward.append(reward)\n",
    "        exp_value=[]\n",
    "        if rand:\n",
    "                action=random.randrange(3)\n",
    "        else:\n",
    "            for k in range(3): # For each action \n",
    "                ex_reward=F_next_state[-1][:-1] \n",
    "                ex_reward.append(k/2)# Normalized action \n",
    "      \n",
    "                prediction=model_game.predict(np.array(ex_reward).reshape(1, -1))  # Will evaluate the value of the next action \n",
    "                ex_reward=[]\n",
    "                exp_value.append(prediction) # First element is first action and so on \n",
    "                \n",
    "            action = np.argmax(exp_value) # Choose the action with the highest value \n",
    "        if len(F_next_state)>3000:\n",
    "            status=1\n",
    "        \n",
    "        j=10\n",
    "        \n",
    "\n",
    "    percentage.append([0,1,1,1,1,1][int(F_reward[-1])])\n",
    "   \n",
    "    print(\"Game number\",u,[\"Fail :(\",\"win\",\"win\",\"win\",\"win\",\"win\"][int(F_reward[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VisualModule import AgentEnvironment\n",
    "from DQN_Agent import NeurosmashAgent\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import os\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "model_output_dir = \"output/model_output/\"\n",
    "\n",
    "if not os.path.exists(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "max_distance = 600\n",
    "show_images = False\n",
    "skip_frames = 15\n",
    "state_size =  13 # agent_pos, enemy_pos, vec_agent, vec_enemy, rel_pos_enemy, done\n",
    "action_size = 3\n",
    "episode_count = 1000\n",
    "batch_size = 32\n",
    "size       = 768         # Please check the Updates section above for more details\n",
    "timescale  = 6           # Please check the Updates section above for more details\n",
    "environment = AgentEnvironment(size=size, timescale=timescale)\n",
    "agent = NeurosmashAgent(state_size = state_size, action_size = action_size, batch_size=batch_size) \n",
    "\n",
    "def compute_reward(standard_reward, distance):\n",
    "    distance_reward = (max_distance-distance)/max_distance\n",
    "    total_reward = (distance_reward + standard_reward)/ 2\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def direction(agent_path, enemy_path):\n",
    "    A_X = (np.array(agent_path[-1]) - np.array(agent_path[-2]))[0]\n",
    "    A_Y = -(np.array(agent_path[-1]) - np.array(agent_path[-2]))[1]\n",
    "    E_X = (np.array(enemy_path[-1]) - np.array(enemy_path[-2]))[0]\n",
    "    E_Y = -(np.array(enemy_path[-1]) - np.array(enemy_path[-2]))[1]\n",
    "    return [A_X,A_Y],[E_X,E_Y]\n",
    "\n",
    "def do_action(action, total_steps, eval_pic):\n",
    "    stopwatch = Stopwatch() \n",
    "    stopwatch.start()\n",
    "    # Normalize the values \n",
    "    info, reward, agent_coord, enemy_coord, _ = environment.actionLoop(action, 0, 1)\n",
    "    # Replace folowing_state with the representation\n",
    "\n",
    "    \n",
    "    stopwatch.stop()\n",
    "    #print(f\"Total time for do action: {stopwatch.duration}\")\n",
    "    if len(environment.agent_path) < 2:\n",
    "        distance = 500 # Initial distance, only for initialisation\n",
    "        agent_direction = [1,0] # By definition of facing each other\n",
    "        enemy_direction = [-1,0]\n",
    "        \n",
    "    else:\n",
    "        distance = np.sqrt(np.square(np.array(list(np.array(agent_coord)- np.array(enemy_coord))).sum(axis=0)))\n",
    "        # Extract all variables \n",
    "        agent_direction, enemy_direction  = direction(environment.agent_path, environment.enemy_path)\n",
    "    \n",
    "    complete_reward = compute_reward(reward, distance)\n",
    "\n",
    "    rel_pos_enemy = np.array(enemy_coord) - np.array(agent_coord)\n",
    "    # Return a value in range 0,1 for following_state\n",
    "    # Nstatus = d[0]\n",
    "    # Ncomplete_reward = d[1]\n",
    "    # Nagent_posx = d[2][0]/700\n",
    "    # Nagent_posy = d[2][1]/700\n",
    "    # Nenemy_posx = d[3][0]/700\n",
    "    # Nenemy_posy = d[3][1]/700\n",
    "    # Nagent_dirx = d[4][0]/60+0.5\n",
    "    # Nagent_diry = d[4][1]/60+0.5\n",
    "    # Nenemy_dirx = d[5][0]/60+0.5\n",
    "    # Nenemy_diry = d[5][1]/60+0.5\n",
    "    # Nenemy_pos_relx = d[7][0]/1400+0.5\n",
    "    # Nenemy_pos_rely = d[7][1]/1400+0.5\n",
    "    # Ndistance = d[6]/700\n",
    "   \n",
    "    d=info, complete_reward, np.array(agent_coord), np.array(enemy_coord), agent_direction, enemy_direction, distance, rel_pos_enemy\n",
    "    following_state = d[0],d[1],d[2][0]/700,d[2][1]/700,d[3][0]/700,d[3][1]/700,\\\n",
    "     d[4][0]/60+0.5,d[4][1]/60+0.5,d[5][0]/60+0.5,d[5][1]/60+0.5,\\\n",
    "     d[7][0]/1400+0.5,d[7][1]/1400+0.5,d[6]/700\n",
    "    \n",
    "    return list(following_state)\n",
    "\n",
    "\n",
    "def init_environment(env, agent_here):\n",
    "    info, reward, state = env.reset() \n",
    "    agent_trajectories = []\n",
    "    enemy_trajectories = []\n",
    "    \n",
    "    small_init_state = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], False]\n",
    "    \n",
    "    #for i in range(3):\n",
    "    action = agent_here.act(small_init_state) # get next action\n",
    "    # action = 3 (if above does not work)\n",
    "    \n",
    "    #pre_state_img = np.flip(np.array(state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "    #step_number_now = i+1\n",
    "    info, complete_reward, agent_pos, enemy_pos, agent_direction, enemy_direction, distance, relative_pos_enemy, next_state = do_action(action, 1, True)  \n",
    "    \n",
    "    #post_state_img = np.flip(np.array(next_state).reshape(3,256,256).transpose(1,2,0),0)\n",
    "\n",
    "    #agent_pos, enemy_pos = env_feat.coord(pre_state_img, post_state_img)\n",
    "    agent_trajectories.append(list(agent_pos))\n",
    "    enemy_trajectories.append(list(enemy_pos))\n",
    "        \n",
    "    return \n",
    "\n",
    "\n",
    "def view(agent_path,enemy_path):\n",
    "\n",
    "    x_a,y_a,x_e,y_e = xyvalues(agent_path,enemy_path)\n",
    "    A_dir=[]\n",
    "    E_dir=[]\n",
    "    Initial_a_dir = [1,0] # By definition they are facing each other\n",
    "    Initial_e_dir = [-1,0]\n",
    "    Game_1 = pd.DataFrame([x_a,y_a,x_e,y_e]).T\n",
    "    Game = Game_1.rename(columns={0: \"A_x_comp\", 1: \"A_y_comp\",2: \"E_x_comp\", 3: \"E_y_comp\"})\n",
    "    interval=1 # The interval difference to consider a trajectory \n",
    "    for i in range(len(Game)-interval):\n",
    "    \n",
    "        Agent_dir = pd.DataFrame(Game.iloc[i]-Game.iloc[i-interval]).iloc[0:2].values\n",
    "        Enemy_dir = pd.DataFrame(Game.iloc[i]-Game.iloc[i-interval]).iloc[2:4].values\n",
    "        \n",
    "        Agent_dir=(Initial_a_dir[0]+normalize(Agent_dir,axis=0)[0][0])/2,(Initial_a_dir[1]+normalize(Agent_dir,axis=0)[1][0])/2\n",
    "        \n",
    "        Enemy_dir=(Initial_e_dir[0]+normalize(Enemy_dir,axis=0)[0][0])/2,(Initial_e_dir[1]+normalize(Enemy_dir,axis=0)[1][0])/2\n",
    "    \n",
    "        A_dir.append(Agent_dir)\n",
    "        E_dir.append(Enemy_dir) # The direction vector is stored in A_dir, E_dir\n",
    "        \n",
    "    for time in range(len(A_dir)):\n",
    "        V = np.array([E_dir[time]])\n",
    "        origin = Game[[\"E_x_comp\",\"E_y_comp\"]].iloc[time].values[0],Game[[\"E_x_comp\",\"E_y_comp\"]].iloc[time].values[1] # origin point\n",
    "        plt.quiver(*origin, V[:,0], V[:,1], color=['b'], scale=30)\n",
    "    \n",
    "        VA = np.array([ A_dir[time]])\n",
    "        origin = Game[[\"A_x_comp\",\"A_y_comp\"]].iloc[time].values[0],Game[[\"A_x_comp\",\"A_y_comp\"]].iloc[time].values[1] # origin point\n",
    "        plt.quiver(*origin, VA[:,0], VA[:,1], color=['r'], scale=30)\n",
    "    \n",
    "    plt.ylim(300, 768)\n",
    "    plt.xlim(0, 768)\n",
    "    plt.savefig('result_{}.png'.format(int(len(full_games_y))))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncoment the model to you want to use and run :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model with \n",
    "NumberOfGames=10\n",
    "epsilon = 0 # zero means model, 1 random.\n",
    "percentage=[]\n",
    " \n",
    "# To load a decent trained model \n",
    "#82.3529411764706 # Model 'nn_82%_50games_8X150n.sav'  loss in training 0.0228, b=100, epoch=2\n",
    "nn_3 = pickle.load(open('nn_82%_50games_8X150n.sav' , 'rb'))\n",
    "#78  #Model 'nn_78%_games_11X150n.sav' loss in training 0.228 b=10000 epoch=1\n",
    "#nn_3 = pickle.load(open('nn_78%_games_11X150n.sav' , 'rb'))\n",
    "#72  #Model 'nn_72%_games_11X150n.sav' loss in training 0.0171 b=100, epoch=1\n",
    "#nn_3 = pickle.load(open('nn_72%_games_11X150n.sav' , 'rb'))\n",
    "#64  #Model 'nn_60%_50games_8X150n.sav' loss in training 0.0208 b=100, epoch=10\n",
    "#nn_3 = pickle.load(open('nn_60%_50games_8X150n.sav' , 'rb'))\n",
    "#66  #Model 'nn_%_games_8X150n.sav' loss in training 0.0192 b=100, epoch=20\n",
    "\n",
    "#nn_3 = pickle.load(open('nn_66%_games_8X150n.sav' , 'rb'))\n",
    "\n",
    "model_game=nn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_reward = 5.491666666666666\n",
      "Game number 1 win\n",
      "Timesteps: 1855\n",
      "F_reward = 5.354166666666667\n",
      "Game number 2 win\n",
      "Timesteps: 905\n",
      "F_reward = 0.49083333333333334\n",
      "Game number 3 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 5.345833333333333\n",
      "Game number 4 win\n",
      "Timesteps: 1427\n",
      "F_reward = 0.41083333333333333\n",
      "Game number 5 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 0.4166666666666667\n",
      "Game number 6 Fail :(\n",
      "Timesteps: 2744\n",
      "F_reward = 0.3641666666666667\n",
      "Game number 7 Fail :(\n",
      "Timesteps: 2622\n",
      "F_reward = 0.4658333333333333\n",
      "Game number 8 Fail :(\n",
      "Timesteps: 2344\n",
      "F_reward = 5.381666666666667\n",
      "Game number 9 win\n",
      "Timesteps: 2191\n",
      "F_reward = 5.420833333333333\n",
      "Game number 10 win\n",
      "Timesteps: 821\n",
      "F_reward = 0.43916666666666665\n",
      "Game number 11 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 4.723333333333334\n",
      "Game number 12 win\n",
      "Timesteps: 1068\n",
      "F_reward = 0.36583333333333334\n",
      "Game number 13 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 0.3983333333333333\n",
      "Game number 14 Fail :(\n",
      "Timesteps: 1322\n",
      "F_reward = 5.4825\n",
      "Game number 15 win\n",
      "Timesteps: 2238\n",
      "F_reward = 0.31916666666666665\n",
      "Game number 16 Fail :(\n",
      "Timesteps: 843\n",
      "F_reward = 0.4091666666666667\n",
      "Game number 17 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 0.4216666666666667\n",
      "Game number 18 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 5.435833333333333\n",
      "Game number 19 win\n",
      "Timesteps: 1671\n",
      "F_reward = 5.0225\n",
      "Game number 20 win\n",
      "Timesteps: 1666\n",
      "F_reward = 0.4116666666666667\n",
      "Game number 21 Fail :(\n",
      "Timesteps: 818\n",
      "F_reward = 0.4275\n",
      "Game number 22 Fail :(\n",
      "Timesteps: 2876\n",
      "F_reward = 0.47583333333333333\n",
      "Game number 23 Fail :(\n",
      "Timesteps: 820\n",
      "F_reward = 0.4608333333333333\n",
      "Game number 24 Fail :(\n",
      "Timesteps: 2239\n",
      "F_reward = 0.4875\n",
      "Game number 25 Fail :(\n",
      "Timesteps: 3001\n",
      "F_reward = 0.4608333333333333\n",
      "Game number 26 Fail :(\n",
      "Timesteps: 583\n",
      "F_reward = 0.3425\n",
      "Game number 27 Fail :(\n",
      "Timesteps: 2629\n",
      "F_reward = 5.408333333333333\n",
      "Game number 28 win\n",
      "Timesteps: 774\n",
      "F_reward = 5.466666666666667\n",
      "Game number 29 win\n",
      "Timesteps: 1108\n",
      "F_reward = 5.486666666666666\n",
      "Game number 30 win\n",
      "Timesteps: 925\n",
      "F_reward = 0.49416666666666664\n",
      "Game number 31 Fail :(\n",
      "Timesteps: 1029\n",
      "F_reward = 0.4675\n",
      "Game number 32 Fail :(\n",
      "Timesteps: 1161\n",
      "F_reward = 5.4975\n",
      "Game number 33 win\n",
      "Timesteps: 2416\n",
      "F_reward = 0.43916666666666665\n",
      "Game number 34 Fail :(\n",
      "Timesteps: 1959\n",
      "F_reward = 5.424166666666666\n",
      "Game number 35 win\n",
      "Timesteps: 1938\n",
      "F_reward = 0.3575\n",
      "Game number 36 Fail :(\n",
      "Timesteps: 1057\n",
      "F_reward = 5.454166666666667\n",
      "Game number 37 win\n",
      "Timesteps: 1125\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ad8d748249f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# action=NewAgent.act(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7c6e332e4a1d>\u001b[0m in \u001b[0;36mdo_action\u001b[0;34m(action, total_steps, eval_pic)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Normalize the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Replace folowing_state with the representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studie/NIPS/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36mactionLoop\u001b[0;34m(self, ACTION, view_image, analyse_frame)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mprevious_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# take last image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_altered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemy_coord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnxt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mstopwatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;31m#print(f\"Total time for coord: {stopwatch.duration}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studie/NIPS/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36mcoord\u001b[0;34m(self, initial_state, action, image)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mstate_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# do action (new \"timestep\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# only get all extra information every 'skip_frames' times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studie/NIPS/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/studie/NIPS/NIPS-project/VisualModule.py\u001b[0m in \u001b[0;36m_receive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Kudos to Jan for the socket.MSG_WAITALL fix!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSG_WAITALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range"
     ]
    }
   ],
   "source": [
    "u=0\n",
    "steps = 0\n",
    "for i in range(NumberOfGames):\n",
    "    environment.reset() \n",
    "    F_next_state=[]\n",
    "    steps = 0\n",
    "    F_reward=[]\n",
    "    prd=[]\n",
    "    u+=1\n",
    "    j=1\n",
    "    status=0\n",
    "    action=0\n",
    "    rand=True\n",
    "    while status==0 or j<10:  \n",
    "        if np.random.rand() <= epsilon:\n",
    "            rand=True\n",
    "        else:\n",
    "            rand=False\n",
    "            \n",
    "        \n",
    "        if rand:\n",
    "            action=random.randrange(3)\n",
    "\n",
    "\n",
    "\n",
    "        next_state = do_action(action,1,True)\n",
    "        # action=NewAgent.act(1)\n",
    "\n",
    "        status=next_state[0] # Normalized status \n",
    "        reward=next_state[1] # reward\n",
    "        next_state = next_state[2:] # State variables\n",
    "        next_state.append(action/2)\n",
    "\n",
    "        F_next_state.append(next_state)\n",
    "        F_reward.append(reward)\n",
    "        exp_value=[]\n",
    "        if rand:\n",
    "                action=random.randrange(3)\n",
    "        else:\n",
    "            for k in range(3): # For each action \n",
    "                ex_reward=F_next_state[-1][:-1] \n",
    "                ex_reward.append(k/2)# Normalized action \n",
    "      \n",
    "                prediction=model_game.predict(np.array(ex_reward).reshape(1, -1))  # Will evaluate the value of the next action \n",
    "                ex_reward=[]\n",
    "                exp_value.append(prediction) # First element is first action and so on \n",
    "                \n",
    "            action = np.argmax(exp_value) # Choose the action with the highest value \n",
    "        \n",
    "        steps += 1\n",
    "        if len(F_next_state)>3000:\n",
    "            status=1\n",
    "        \n",
    "        j=10\n",
    "        \n",
    "        \n",
    "    print(percentage)\n",
    "    percentage.append([0,1,1,1,1,1][int(F_reward[-1])])\n",
    "    print(f\"F_reward = {F_reward[-1]}\")\n",
    "    print(\"Game number\",u,[\"Fail :(\",\"win\",\"win\",\"win\",\"win\",\"win\"][int(F_reward[-1])])\n",
    "    print(f\"Timesteps: {steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
